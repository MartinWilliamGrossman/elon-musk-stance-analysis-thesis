{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import zstandard\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = 'PUT_NAME_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions = pd.read_csv(f\"reddit_data/{subreddit_name}_submissions_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "input_file = f\"subreddits24\\{subreddit_name}_comments.zst\" \n",
    "output_file = f\"subreddits24\\{subreddit_name}_comments_filtered\" \n",
    "\n",
    "output_format = \"csv\" \n",
    "\n",
    "field = None\n",
    "values = []\n",
    "values_file = None\n",
    "exact_match = False\n",
    "\n",
    "def write_headers_csv(writer, is_submission):\n",
    "    if is_submission:\n",
    "        headers = ['Score', 'Date', 'Title', 'Author', 'URL', 'Selftext', 'External URL','ID']\n",
    "    else:\n",
    "        headers = ['Score', 'Date', 'Comment ID', 'Author', 'URL', 'Body', 'ID', 'Post ID']\n",
    "    writer.writerow(headers)\n",
    "\n",
    "def write_line_csv(writer, obj, is_submission):\n",
    "    output_list = []\n",
    "    output_list.append(str(obj['score']))\n",
    "    output_list.append(datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    if is_submission:\n",
    "        output_list.append(obj['title'])\n",
    "    else:\n",
    "        output_list.append(obj['id'])\n",
    "        \n",
    "    output_list.append(f\"u/{obj['author']}\")\n",
    "    \n",
    "    if 'permalink' in obj:\n",
    "        output_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "    else:\n",
    "        output_list.append(f\"https://www.reddit.com/r/{obj['subreddit']}/comments/{obj['link_id'][3:]}/_/{obj['id']}\")\n",
    "    \n",
    "    if is_submission:\n",
    "        # Add selftext as its own column\n",
    "        if 'selftext' in obj:\n",
    "            output_list.append(obj['selftext'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "        \n",
    "        # Add external URL as its own column\n",
    "        if 'url' in obj:\n",
    "            output_list.append(obj['url'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "\n",
    "        if 'id' in obj:\n",
    "            output_list.append(obj['id'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "\n",
    "    else:\n",
    "        output_list.append(obj['body'])\n",
    "        \n",
    "        if 'id' in obj:\n",
    "            output_list.append(obj['id'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "\n",
    "        if 'parent_id' in obj:\n",
    "            output_list.append(obj['parent_id'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "        \n",
    "    writer.writerow(output_list)\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "    chunk = reader.read(chunk_size)\n",
    "    bytes_read += chunk_size\n",
    "    if previous_chunk is not None:\n",
    "        chunk = previous_chunk + chunk\n",
    "    try:\n",
    "        return chunk.decode()\n",
    "    except UnicodeDecodeError:\n",
    "        if bytes_read > max_window_size:\n",
    "            raise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "        return read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "    with open(file_name, 'rb') as file_handle:\n",
    "        buffer = ''\n",
    "        reader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "        while True:\n",
    "            chunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line.strip(), file_handle.tell()\n",
    "\n",
    "            buffer = lines[-1]\n",
    "\n",
    "        reader.close()\n",
    "\n",
    "def process_file(input_file, output_file, output_format, field, values, submission_ids, single_field, exact_match):\n",
    "    output_path = f\"{output_file}.{output_format}\"\n",
    "    is_submission = \"submission\" in input_file\n",
    "    print(f\"Processing {input_file} to {output_path}\")\n",
    "    handle = open(output_path, 'w', encoding='UTF-8', newline='')\n",
    "    writer = csv.writer(handle)\n",
    "    write_headers_csv(writer, is_submission)\n",
    "\n",
    "    file_size = os.stat(input_file).st_size\n",
    "    matched_lines = 0\n",
    "    total_lines = 0\n",
    "    for line, file_bytes_processed in read_lines_zst(input_file):\n",
    "        total_lines += 1\n",
    "        if total_lines % 1000000 == 0:\n",
    "            print(f\"Progress: {total_lines:,} lines, {matched_lines:,} matches, {(file_bytes_processed / file_size) * 100:.0f}%\")\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            \n",
    "            # Filter based on parent_id matching submission IDs\n",
    "            if 'parent_id' in obj:\n",
    "                parent_id = obj['parent_id']\n",
    "                if isinstance(parent_id, str) and \"_\" in parent_id:\n",
    "                    # Extract the ID part after the prefix\n",
    "                    parent_id_clean = parent_id.split(\"_\", 1)[1]\n",
    "                    \n",
    "                    # Check if this ID matches any in the submission_ids list\n",
    "                    if parent_id_clean not in submission_ids:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if field is not None:\n",
    "                try:\n",
    "                    field_value = obj[field].lower()\n",
    "                    matched = False\n",
    "                    for value in values:\n",
    "                        if (exact_match and value == field_value) or (not exact_match and value in field_value):\n",
    "                            matched = True\n",
    "                            break\n",
    "                    if not matched:\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            matched_lines += 1\n",
    "            write_line_csv(writer, obj, is_submission)\n",
    "        except (KeyError, json.JSONDecodeError):\n",
    "            continue\n",
    "\n",
    "    handle.close()\n",
    "    print(f\"Complete: {total_lines:,} lines processed, {matched_lines:,} matches\")\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    # Get submission IDs from DataFrame and convert to strings\n",
    "    submission_ids = [str(id_val) for id_val in df_submissions['ID'].tolist()]\n",
    "    print(f\"Loaded {len(submission_ids)} submission IDs to filter by\")\n",
    "    \n",
    "    if single_field is not None:\n",
    "        output_format = \"txt\"\n",
    "\n",
    "    if values_file is not None:\n",
    "        values = []\n",
    "        with open(values_file, 'r') as values_handle:\n",
    "            for value in values_handle:\n",
    "                values.append(value.strip().lower())\n",
    "    else:\n",
    "        values = [value.lower() for value in values]\n",
    "\n",
    "    input_files = []\n",
    "    if os.path.isdir(input_file):\n",
    "        if not os.path.exists(output_file):\n",
    "            os.makedirs(output_file)\n",
    "        for file in os.listdir(input_file):\n",
    "            if not os.path.isdir(file) and file.endswith(\".zst\"):\n",
    "                input_name = os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "                input_files.append((os.path.join(input_file, file), os.path.join(output_file, input_name)))\n",
    "    else:\n",
    "        input_files.append((input_file, output_file))\n",
    "    \n",
    "    print(f\"Processing {len(input_files)} files\")\n",
    "    for file_in, file_out in input_files:\n",
    "        try:\n",
    "            process_file(file_in, file_out, output_format, field, values, submission_ids, single_field, exact_match)\n",
    "        except Exception as err:\n",
    "            print(f\"Error processing {file_in}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'subreddits24/{subreddit_name}_comments_filtered.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV\n",
    "output_filepath = f'reddit_data/{subreddit_name}_comments_filtered.csv' \n",
    "df.to_csv(output_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
