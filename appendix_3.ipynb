{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"reddit_data\"\n",
    "\n",
    "politics_comments_filtered = pd.read_csv(os.path.join(folder_path, \"politics_comments_filtered.csv\"))\n",
    "democrats_comments_filtered = pd.read_csv(os.path.join(folder_path, \"democrats_comments_filtered.csv\"))\n",
    "spacex_comments_filtered = pd.read_csv(os.path.join(folder_path, \"spacex_comments_filtered.csv\"))\n",
    "teslamotors_comments_filtered = pd.read_csv(os.path.join(folder_path, \"teslamotors_comments_filtered.csv\"))\n",
    "PoliticalDiscussion_comments_filtered = pd.read_csv(os.path.join(folder_path, \"PoliticalDiscussion_comments_filtered.csv\"))\n",
    "Libertarian_comments_filtered = pd.read_csv(os.path.join(folder_path, \"Libertarian_comments_filtered.csv\"))\n",
    "Conservative_comments_filtered = pd.read_csv(os.path.join(folder_path, \"Conservative_comments_filtered.csv\"))\n",
    "technology_comments_filtered = pd.read_csv(os.path.join(folder_path, \"technology_comments_filtered.csv\"))\n",
    "AskThe_Donald_comments_filtered = pd.read_csv(os.path.join(folder_path, \"AskThe_Donald_comments_filtered.csv\"))\n",
    "technology_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"technology_submissions_filtered.csv\"))\n",
    "teslamotors_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"teslamotors_submissions_filtered.csv\"))\n",
    "spacex_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"spacex_submissions_filtered.csv\"))\n",
    "AskThe_Donald_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"AskThe_Donald_submissions_filtered.csv\"))\n",
    "Conservative_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"Conservative_submissions_filtered.csv\"))\n",
    "democrats_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"democrats_submissions_filtered.csv\"))\n",
    "Libertarian_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"Libertarian_submissions_filtered.csv\"))\n",
    "PoliticalDiscussion_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"PoliticalDiscussion_submissions_filtered.csv\"))\n",
    "politics_submissions_filtered = pd.read_csv(os.path.join(folder_path, \"politics_submissions_filtered.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political orientation mapping\n",
    "subreddit_orientation = {\n",
    "    \"politics\": \"Left\",\n",
    "    \"democrats\": \"Left\",\n",
    "    \"PoliticalDiscussion\": \"Left\",\n",
    "    \"Libertarian\": \"Right\",\n",
    "    \"Conservative\": \"Right\",\n",
    "    \"AskThe_Donald\": \"Right\",\n",
    "    \"spacex\": \"Neutral\",\n",
    "    \"technology\": \"Neutral\",\n",
    "    \"teslamotors\": \"Neutral\"\n",
    "}\n",
    "\n",
    "# List of submission dataframes\n",
    "submission_dataframes = [\n",
    "    (\"politics\", politics_submissions_filtered),\n",
    "    (\"democrats\", democrats_submissions_filtered),\n",
    "    (\"PoliticalDiscussion\", PoliticalDiscussion_submissions_filtered),\n",
    "    (\"Libertarian\", Libertarian_submissions_filtered),\n",
    "    (\"Conservative\", Conservative_submissions_filtered),\n",
    "    (\"AskThe_Donald\", AskThe_Donald_submissions_filtered),\n",
    "    (\"spacex\", spacex_submissions_filtered),\n",
    "    (\"technology\", technology_submissions_filtered),\n",
    "    (\"teslamotors\", teslamotors_submissions_filtered)\n",
    "]\n",
    "\n",
    "# Create a list to hold the modified dataframes\n",
    "modified_dfs = []\n",
    "\n",
    "# Iterate through the submission dataframes and add the subreddit and orientation columns\n",
    "for subreddit, df in submission_dataframes:\n",
    "    df[\"subreddit\"] = subreddit\n",
    "    df[\"political_orientation\"] = subreddit_orientation[subreddit]\n",
    "    modified_dfs.append(df)\n",
    "\n",
    "# Concatenate all the modified dataframes into one\n",
    "combined_submissions_df = pd.concat(modified_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of comment dataframes\n",
    "comment_dataframes = [\n",
    "    (\"politics\", politics_comments_filtered),\n",
    "    (\"democrats\", democrats_comments_filtered),\n",
    "    (\"PoliticalDiscussion\", PoliticalDiscussion_comments_filtered),\n",
    "    (\"Libertarian\", Libertarian_comments_filtered),\n",
    "    (\"Conservative\", Conservative_comments_filtered),\n",
    "    (\"AskThe_Donald\", AskThe_Donald_comments_filtered),\n",
    "    (\"spacex\", spacex_comments_filtered),\n",
    "    (\"technology\", technology_comments_filtered),\n",
    "    (\"teslamotors\", teslamotors_comments_filtered)\n",
    "]\n",
    "\n",
    "# Create a list to hold the modified dataframes\n",
    "modified_dfs = []\n",
    "\n",
    "# Iterate through the comment dataframes and add the subreddit and orientation columns\n",
    "for subreddit, df in comment_dataframes:\n",
    "    df[\"subreddit\"] = subreddit\n",
    "    df[\"political_orientation\"] = subreddit_orientation[subreddit]\n",
    "    modified_dfs.append(df)\n",
    "\n",
    "# Concatenate all the modified dataframes into one\n",
    "combined_comments_df = pd.concat(modified_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_comments_df_selected = combined_comments_df[[\"Date\", \"Body\", \"Post ID\", \"subreddit\", \"political_orientation\"]]\n",
    "combined_submissions_df_selected = combined_submissions_df[[\"Date\", \"Title\", \"Selftext\", \"ID\", \"subreddit\", \"political_orientation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(comments, submissions):\n",
    "    comments = comments.rename(columns={\"Post ID\": \"ID\", \"Body\": \"Text\"})\n",
    "    submissions = submissions.rename(columns={\"Selftext\": \"Text\"})\n",
    "    return pd.concat([comments[[\"ID\", \"Date\", \"Text\", \"subreddit\", \"political_orientation\"]], \n",
    "                      submissions[[\"ID\", \"Date\", \"Text\", \"subreddit\", \"political_orientation\"]]], \n",
    "                     ignore_index=True)\n",
    "\n",
    "combined_df = combine_dfs(combined_comments_df_selected, combined_submissions_df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[combined_df['Text'] != '[removed]']\n",
    "\n",
    "combined_df = combined_df[combined_df['Text'].notna()]\n",
    "\n",
    "combined_df = combined_df[~combined_df['Text'].str.contains('I am a bot, and this action was performed automatically.', na=False)]\n",
    "\n",
    "combined_df = combined_df[combined_df['Text'].str.len() >= 10]\n",
    "\n",
    "combined_df = combined_df[~combined_df['Text'].str.match(r'^(?:http|https)://\\S+$', na=False)]\n",
    "\n",
    "combined_df = combined_df[~combined_df['Text'].str.match(r'^!?\\[gif\\]\\([^)]*\\)$', na=False, flags=re.IGNORECASE)]\n",
    "\n",
    "combined_df = combined_df[~combined_df['Text'].str.contains(\"Thanks for contributing! Unfortunately your submission has been removed\", na=False, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_excel(\"reddit_data/combined_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: (900, 5)\n",
      "Remaining data shape: (71459, 5)\n",
      "Original data shape: (72359, 5)\n",
      "\n",
      "Sample distribution by subreddit:\n",
      "subreddit\n",
      "politics               100\n",
      "democrats              100\n",
      "PoliticalDiscussion    100\n",
      "Libertarian            100\n",
      "Conservative           100\n",
      "AskThe_Donald          100\n",
      "spacex                 100\n",
      "technology             100\n",
      "teslamotors            100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert Date to datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(combined_df['Date']):\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "\n",
    "# Get list of unique subreddits\n",
    "subreddits = combined_df['subreddit'].unique()\n",
    "\n",
    "sample_data = pd.DataFrame()\n",
    "\n",
    "# For each subreddit, sample 100 rows distributed by date\n",
    "for subreddit in subreddits:\n",
    "    subreddit_df = combined_df[combined_df['subreddit'] == subreddit]\n",
    "    \n",
    "    dates = sorted(subreddit_df['Date'].unique())\n",
    "    \n",
    "    samples_per_date = 100 // len(dates)\n",
    "    remainder = 100 % len(dates)\n",
    "    \n",
    "    subreddit_sample = pd.DataFrame()\n",
    "    \n",
    "    for i, date in enumerate(dates):\n",
    "        date_df = subreddit_df[subreddit_df['Date'] == date]\n",
    "        \n",
    "        n_samples = samples_per_date + (1 if i < remainder else 0)\n",
    "        \n",
    "        n_samples = min(n_samples, len(date_df))\n",
    "        \n",
    "        date_sample = date_df.sample(n=n_samples, random_state=42)\n",
    "        subreddit_sample = pd.concat([subreddit_sample, date_sample])\n",
    "    \n",
    "    sample_data = pd.concat([sample_data, subreddit_sample])\n",
    "\n",
    "combined_data_without_sample = combined_df[~combined_df.index.isin(sample_data.index)]\n",
    "\n",
    "sample_data.to_excel('reddit_data/sample_data.xlsx', index=False)\n",
    "combined_data_without_sample.to_excel('reddit_data/combined_data_without_sample.xlsx', index=False)\n",
    "\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(f\"Remaining data shape: {combined_data_without_sample.shape}\")\n",
    "print(f\"Original data shape: {combined_df.shape}\")\n",
    "\n",
    "print(sample_data['subreddit'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
