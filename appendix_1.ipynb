{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import zstandard\n",
    "import os\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = 'PUT_NAME_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_18548\\1391284279.py:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  input_file = f\"subreddits24\\{subreddit_name}_submissions.zst\"\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_18548\\1391284279.py:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  output_file = f\"subreddits24\\{subreddit_name}_submissions_filtered\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 files\n",
      "Processing subreddits24\\AskThe_Donald_submissions.zst to subreddits24\\AskThe_Donald_submissions_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_18548\\1391284279.py:125: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  created = datetime.utcfromtimestamp(int(obj['created_utc']))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: 106,580 lines processed, 3,942 matches\n"
     ]
    }
   ],
   "source": [
    "input_file = f\"subreddits24\\{subreddit_name}_submissions.zst\" \n",
    "output_file = f\"subreddits24\\{subreddit_name}_submissions_filtered\" \n",
    "\n",
    "output_format = \"csv\" \n",
    "\n",
    "single_field = None \n",
    "write_bad_lines = False \n",
    "\n",
    "from_date = datetime.strptime(\"2024-03-13\", \"%Y-%m-%d\") \n",
    "to_date = datetime.strptime(\"2024-12-13\", \"%Y-%m-%d\") \n",
    "\n",
    "field = None\n",
    "values = []\n",
    "values_file = None\n",
    "exact_match = False\n",
    "\n",
    "def write_headers_csv(writer, is_submission):\n",
    "    if is_submission:\n",
    "        headers = ['Score', 'Date', 'Title', 'Author', 'URL', 'Selftext', 'External URL','ID']\n",
    "    else:\n",
    "        headers = ['Score', 'Date', 'Comment ID', 'Author', 'URL', 'Body']\n",
    "    writer.writerow(headers)\n",
    "\n",
    "def write_line_csv(writer, obj, is_submission):\n",
    "    output_list = []\n",
    "    output_list.append(str(obj['score']))\n",
    "    output_list.append(datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    if is_submission:\n",
    "        output_list.append(obj['title'])\n",
    "    else:\n",
    "        output_list.append(obj['id'])\n",
    "        \n",
    "    output_list.append(f\"u/{obj['author']}\")\n",
    "    \n",
    "    if 'permalink' in obj:\n",
    "        output_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "    else:\n",
    "        output_list.append(f\"https://www.reddit.com/r/{obj['subreddit']}/comments/{obj['link_id'][3:]}/_/{obj['id']}\")\n",
    "    \n",
    "    if is_submission:\n",
    "        # Add selftext as its own column\n",
    "        if 'selftext' in obj:\n",
    "            output_list.append(obj['selftext'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "        \n",
    "        # Add external URL as its own column\n",
    "        if 'url' in obj:\n",
    "            output_list.append(obj['url'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "\n",
    "        if 'id' in obj:\n",
    "            output_list.append(obj['id'])\n",
    "        else:\n",
    "            output_list.append(\"\")\n",
    "\n",
    "    else:\n",
    "        output_list.append(obj['body'])\n",
    "        \n",
    "    writer.writerow(output_list)\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "    chunk = reader.read(chunk_size)\n",
    "    bytes_read += chunk_size\n",
    "    if previous_chunk is not None:\n",
    "        chunk = previous_chunk + chunk\n",
    "    try:\n",
    "        return chunk.decode()\n",
    "    except UnicodeDecodeError:\n",
    "        if bytes_read > max_window_size:\n",
    "            raise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "        return read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "    with open(file_name, 'rb') as file_handle:\n",
    "        buffer = ''\n",
    "        reader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "        while True:\n",
    "            chunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line.strip(), file_handle.tell()\n",
    "\n",
    "            buffer = lines[-1]\n",
    "\n",
    "        reader.close()\n",
    "\n",
    "def process_file(input_file, output_file, output_format, field, values, from_date, to_date, single_field, exact_match):\n",
    "    output_path = f\"{output_file}.{output_format}\"\n",
    "    is_submission = \"submission\" in input_file\n",
    "    print(f\"Processing {input_file} to {output_path}\")\n",
    "    handle = open(output_path, 'w', encoding='UTF-8', newline='')\n",
    "    writer = csv.writer(handle)\n",
    "    write_headers_csv(writer, is_submission)\n",
    "\n",
    "    file_size = os.stat(input_file).st_size\n",
    "    matched_lines = 0\n",
    "    total_lines = 0\n",
    "    for line, file_bytes_processed in read_lines_zst(input_file):\n",
    "        total_lines += 1\n",
    "        if total_lines % 1000000 == 0:\n",
    "            print(f\"Progress: {total_lines:,} lines, {matched_lines:,} matches, {(file_bytes_processed / file_size) * 100:.0f}%\")\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            created = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
    "\n",
    "            if created < from_date or created > to_date:\n",
    "                continue\n",
    "\n",
    "            if field is not None:\n",
    "                try:\n",
    "                    field_value = obj[field].lower()\n",
    "                    matched = False\n",
    "                    for value in values:\n",
    "                        if (exact_match and value == field_value) or (not exact_match and value in field_value):\n",
    "                            matched = True\n",
    "                            break\n",
    "                    if not matched:\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            matched_lines += 1\n",
    "            write_line_csv(writer, obj, is_submission)\n",
    "        except (KeyError, json.JSONDecodeError):\n",
    "            continue\n",
    "\n",
    "    handle.close()\n",
    "    print(f\"Complete: {total_lines:,} lines processed, {matched_lines:,} matches\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if single_field is not None:\n",
    "        output_format = \"txt\"\n",
    "\n",
    "    if values_file is not None:\n",
    "        values = []\n",
    "        with open(values_file, 'r') as values_handle:\n",
    "            for value in values_handle:\n",
    "                values.append(value.strip().lower())\n",
    "    else:\n",
    "        values = [value.lower() for value in values]\n",
    "\n",
    "    input_files = []\n",
    "    if os.path.isdir(input_file):\n",
    "        if not os.path.exists(output_file):\n",
    "            os.makedirs(output_file)\n",
    "        for file in os.listdir(input_file):\n",
    "            if not os.path.isdir(file) and file.endswith(\".zst\"):\n",
    "                input_name = os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "                input_files.append((os.path.join(input_file, file), os.path.join(output_file, input_name)))\n",
    "    else:\n",
    "        input_files.append((input_file, output_file))\n",
    "    \n",
    "    print(f\"Processing {len(input_files)} files\")\n",
    "    for file_in, file_out in input_files:\n",
    "        try:\n",
    "            process_file(file_in, file_out, output_format, field, values, from_date, to_date, single_field, exact_match)\n",
    "        except Exception as err:\n",
    "            print(f\"Error processing {file_in}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'subreddits24/{subreddit_name}_submissions_filtered.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_elon_musk_posts(df):\n",
    "\n",
    "    def contains_elon_musk(row):\n",
    "        for col in ['Title', 'Selftext', 'External URL']:\n",
    "            if pd.notna(row[col]) and (\n",
    "                'elon' in str(row[col]).lower() or 'musk' in str(row[col]).lower()\n",
    "            ):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    filtered_df = df[df.apply(contains_elon_musk, axis=1)].copy()\n",
    "    return filtered_df\n",
    "\n",
    "elon_musk_df = filter_elon_musk_posts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV\n",
    "output_filepath = f'reddit_data/{subreddit_name}_submissions_filtered.csv' \n",
    "elon_musk_df.to_csv(output_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
